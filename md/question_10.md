# Билет №10
## Вопрос 1: Optimizers: Adam, NAdam.
> ### Chain rule
> $Chain\ rule$ -- производная сложной функции.
> 
> **Цепное правило (правило дифференцирования сложной функции)** позволяет вычислить производную композиции двух и более функций на основе индивидуальных производных. Если функция $f$ имеет производную в точке $x_{0}$, а функция $g$ имеет производную в точке $y_{0}=f(x_{0})$, то сложная функция $h(x)=g(f(x))$ также имеет производную в точке $x_{0}$.
> 
> В обозначениях Лейбница цепное правило для вычисления производной функции $y=y(x)$, где $x=x(t)$, принимает следующий вид:
> 
> $$\frac{dy}{dt} = \frac{dy}{dx} \cdot \frac{dt}{dt}$$
> ### Backpropagation
> $Backpropagation$ -- метод обновления весов сеток.
> [Источник того, что ниже -- википедия.](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BE%D0%B1%D1%80%D0%B0%D1%82%D0%BD%D0%BE%D0%B3%D0%BE_%D1%80%D0%B0%D1%81%D0%BF%D1%80%D0%BE%D1%81%D1%82%D1%80%D0%B0%D0%BD%D0%B5%D0%BD%D0%B8%D1%8F_%D0%BE%D1%88%D0%B8%D0%B1%D0%BA%D0%B8)
> 
> **Метод обратного распространения ошибки** — метод вычисления градиента, который используется при обновлении весов многослойного перцептрона.
> Основная идея этого метода состоит в распространении сигналов ошибки от выходов сети к её входам, в направлении, обратном прямому распространению сигналов в обычном режиме работы.
> #### Описание работы
> Алгоритм обратного распространения ошибки применяется для многослойного перцептрона. У сети есть множество входов $x_{1},...,x_{n}$, множество выходов $Outputs$ и множество внутренних узлов. Перенумеруем все узлы (включая входы и выходы) числами от $1$ до $N$ (сквозная нумерация, вне зависимости от топологии слоёв). Обозначим через  $w_{i,j}$ вес, стоящий на ребре, соединяющем $i$-й и $j$-й узлы, а через $o_i$ — выход $i$-го узла. Если нам известен обучающий пример (правильные ответы сети $t_k$, $k \in \mathrm{Outputs}$), то функция ошибки, полученная по методу наименьших квадратов, выглядит так:
>
>$$E(\{w_{i,j}\}) = \tfrac{1}{2} \!\! \sum_{k \in \mathrm{Outputs}} \!\!\! (t_k - o_k)^2 $$
>
> Как модифицировать веса? Мы будем реализовывать стохастический градиентный спуск, то есть будем подправлять веса после каждого обучающего примера и, таким образом, «двигаться» в многомерном пространстве весов. Чтобы «добраться» до минимума ошибки, нам нужно «двигаться» в сторону, противоположную градиенту, то есть, на основании каждой группы правильных ответов, добавлять к каждому весу $w_{i,j}$
>
> $$\Delta w_{i,j} = -\eta \frac {\partial E}{\partial w_{i,j}},$$
>
> где $0 < \eta < 1$ — множитель, задающий скорость «движения»
>
>Производная считается следующим образом. Пусть сначала $j \in \mathrm{Outputs}$, то есть интересующий нас вес входит в нейрон последнего уровня. Сначала отметим, что $w_{i,j}$ влияет на выход сети только как часть суммы  $S_j = \sum_{i} w_{i,j}x_{i}$, где сумма берётся по входам $j$-го узла. Поэтому
>
> $$\cfrac{\partial E}{\partial w_{i,j}} = \cfrac{\partial E}{\partial S_j}\, \cfrac{\partial S_j}{\partial w_{i,j}} = x_{i} \cfrac{\partial E}{\partial S_j} $$
>
> Аналогично, $S_j$ влияет на общую ошибку только в рамках выхода $j$-го узла $o_j$ (напоминаем, что это выход всей сети). Поэтому
>
>$$\cfrac{\partial E}{\partial S_j} = \cfrac{\partial E}{\partial o_j}\,\cfrac{\partial o_j}{\partial S_j} = \\ =\left (\cfrac{\partial}{\partial o_j}\,\cfrac{1}{2}\sum_{k \in \mathrm{Outputs}}(t_k - o_k)^2 \right ) \!\! \left (\cfrac{\partial \operatorname{f}(S)}{\partial S}\mid_{S=S_j} \right) = \\
\\ = \left ( \cfrac{1}{2}\, \cfrac{\partial}{\partial o_j}(t_j - o_j)^2 \right) (o_j(1 - o_j)) 2\alpha = - 2 \alpha o_j(1 - o_j)(t_j - o_j),$$
>
>где $f(S)$ — соответствующая сигмоида, в данном случае — экспоненциальная
>
>Если же $j$-й узел — не на последнем уровне, то у него есть выходы; обозначим их через $Children(j)$. В этом случае
>
>$$\cfrac{\partial E}{\partial S_j} = \sum_{k \in \mathrm{Children}(j)} \cfrac{\partial E}{\partial S_k}\, \cfrac{\partial S_k}{\partial S_j},$$
>и
>
>$$\cfrac{\partial S_k}{\partial S_j} = \cfrac{\partial S_k}{\partial o_j}\, \cfrac{\partial o_j}{\partial S_j} = w_{j,k} \cfrac{\partial o_j}{\partial S_j} = 2\alpha w_{j,k}o_j(1 - o_j).$$
>
>Но $\cfrac{\partial E}{\partial S_k}$ — это в точности аналогичная поправка, но вычисленная для узла следующего уровня. Будем обозначать её через $\delta _k$ — от $\Delta _k$ она отличается отсутствием множителя $(-\eta x_{i,j})$. Поскольку мы научились вычислять поправку для узлов последнего уровня и выражать поправку для узла более низкого уровня через поправки более высокого, можно уже писать алгоритм. Именно из-за этой особенности вычисления поправок алгоритм называется алгоритмом обратного распространения ошибки ($backpropagation$).
>
>Краткое резюме проделанной работы:
>* для узла последнего уровня:
>$$\delta _j = -2\alpha o_j(1 - o_j)(t_j - o_j)$$
>* для внутреннего узла сети:
>$$\delta _j = 2\alpha o_j(1 - o_j) \!\! \sum_{k \in \mathrm{Children}(j)} \!\! \delta _k w_{j,k}$$
>* для всех узлов:
>$$\Delta w_{i,j} = -\eta \delta _j o_{i},$$
>где $o_{i}$ это тот же $x_{i}$ в формуле для $\cfrac{\partial E}>{\partial w_{i,j}}$.
>
>[Источник того, что ниже - лекции.](https://docs.google.com/document/d/1HXz9Q7sp50WDAd3Fuk0eqzikXN90KmClK8fkG-2y0Iw/edit#)
>
>Итак, мы посчитали градиенты по каждому весу, но это градиенты функции ошибки, ее мы хотим минимизировать, так что веса будем двигать в сторону, противоположную градиенту. Замечание: могут встречаться оба варианта: подвигать по градиенту (если мы максимизируем какой-нибудь “gain”) или против (если минимизируем loss).
>Сам алгоритм:
>* инициализировать веса $w_{i, j}$ случайными величинами
>* $forward\ step$ - получаем $o_j$
>* $backward\ step$ - получаем $\delta_j$
>* обновляем веса
>
>Этот алгоритм лучше запускать несколько раз с разными начальными весами, потому что начиная с разных точек в пространстве весов и значения функции потерь мы можем скатываться в разные минимумы.

### Оптимизаторы

#### Adam

[Источник того, что ниже - хабр.](https://habr.com/post/318970/)

$Adam — adaptive\ moment\ estimation$

$Adam -$ это оптимизационный алгоритм.

> **Вспомним основы оптимизационных алгоритмов, основанных на накоплении:**
> Сама по себе идея методов с накоплением импульса до очевидности проста: «Если мы некоторое время движемся в определённом направлении, то, вероятно, нам следует туда двигаться некоторое время и в будущем». Для этого нужно уметь обращаться к недавней истории изменений каждого параметра. Можно хранить последние $n$ экземпляров $\Delta w$ и на каждом шаге по-честному считать среднее, но такой подход занимает слишком много памяти для больших $n$. К счастью, нам и не нужно точное среднее, а лишь оценку, поэтому воспользуемся экспоненциальным скользящим средним.
>
> $$v_t = \gamma \cdot v_{t-1} + (1-\gamma) x$$
>
> Чтобы накопить что-нибудь, будем умножать уже накопленное значение на коэффициент сохранения $0 < \gamma < 1$ и прибавлять очередную величину, умноженную на $1-\gamma$. Чем ближе $\gamma$ к единице, тем больше окно накопления и сильнее сглаживание — история $x$ начинает влиять сильнее, чем каждое очередное $x$. Если $x = 0$ c какого-то момента, $v_t$ затухают по геометрической прогрессии, экспоненциально, отсюда и название. Применим экспоненциальное бегущее среднее, чтобы накапливать градиент целевой функции нашей сети:
>
> $$v_t = \gamma \cdot v_{t-1} + \eta \nabla_w J( w)$$
> 
>$$w_{t+1} = w_{t} - v_t$$
>
> [Другой источник, другие обозначения:](http://ruder.io/optimizing-gradient-descent/)
> $$m_t =  \gamma \cdot m_{t-1} + \eta \cdot g_t\\
> w_{t+1} = w_t - m_t \\
> w_{t + 1} = w_t - (\gamma \cdot m_{t-1} + \eta \cdot g_t)$$

Он сочетает в себе и идею накопления движения и идею более слабого обновления весов для типичных признаков.

От Нестерова $Adam$ отличается тем, что мы накапливаем не $\Delta w$, а значения градиента.

Кроме того, мы хотим знать, как часто градиент изменяется. Авторы алгоритма предложили для этого оценивать ещё и среднюю нецентрированную дисперсию:

$$v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2,$$

где $g_t = \nabla_w J( w_t ),\ \ \ J( w )$ - целевая функция.

Легко заметить, что это уже знакомый нам $E[g^2]_t$, то есть это бегущее среднее в момент $t$:
$$E[g^2]_t = \gamma E[g^2]_{t-1} + (1 - \gamma) g^2_t,$$
так что по сути тут нет отличий от $RMSProp$.

Обновление моментума будет по формуле:
$$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$$

Важное отличие состоит в начальной калибровке $m_t$ и $v_t$: они страдают от той же проблемы, что и $E[g^2]_t$ в $RMSProp$:
если задать нулевое начальное значение, то они будут долго накапливаться, особенно при большом окне накопления $(0 \ll \beta_1 < 1,\ 0 \ll \beta_2 < 1)$,
а какие-то изначальные значения — это ещё два гиперпараметра.

Никто не хочет ещё два гиперпараметра, так что мы искусственно увеличиваем $m_t$ и $v_t$ на первых шагах:
* для $m_t$ примерно на $0 < t < 10$
* для $v_t$ примерно на $0 < t < 1000$

$$\hat{m}_t = \frac{m_t}{1 - \beta^t_1}, \;\\
\hat{v}_t = \frac{v_t}{1 - \beta^t_2}$$

В итоге, правило обновления весов получается следующим:
$$w_{t+1} = w_{t} - \dfrac{\eta}{\sqrt{\hat{v}_t + \varepsilon}} \hat{m}_t$$

Авторы Adam предлагают в качестве значений по умолчанию: $$\beta_1 = 0.9, \beta_2 = 0.999, \varepsilon = 10^{-8}$$ и утверждают, что алгоритм выступает лучше или примерно так же, как и все предыдущие алгоритмы на широком наборе датасетов за счёт начальной калибровки.

#### Nadam

$Nesterov\ -\ accelerated\ Adaptive\ Moment\ Estimation$
[Источник того, что ниже - лекции.](https://docs.google.com/document/d/1HXz9Q7sp50WDAd3Fuk0eqzikXN90KmClK8fkG-2y0Iw/edit#heading=h.pfqx4aqepwsx)

Раз уж мы в предыдущем методе воспользовались импульсом, то можно и про Нестерова тоже вспомнить. Итого, заменяем в предыдущем Adam-e переход по сумме векторов импульса и  градиента на последовательный переход: сначала по вектору импульса, потом по вектору градиента в новой точке.


Формулы для $Nadam:$

Сначала правило обновления весов по $Adam:$

<p align="center">
  <img src = "https://github.com/natalymr/dl-notes/blob/master/pictures/question_10/adam.png?raw=true">
</p>

Теперь правило обновления весов по $Nadam:$

<p align="center">
  <img src = "https://github.com/natalymr/dl-notes/blob/master/pictures/question_10/nadam.png?raw=true">
</p>

[Источник картинок](http://cs229.stanford.edu/proj2015/054_report.pdf)
[Источник, где можно почитать формулки, но... лучше не читать.](http://ruder.io/optimizing-gradient-descent/)



## Вопрос 2: Object detection. IoU, mAP. R-CNN. Fast R-CNN.