# Билет №6
## Вопрос 1: Optimizers: Adagrad, Adadelta, RMSProp.

### Chain rule
$Chain\ rule$ -- производная сложной функции.

**Цепное правило (правило дифференцирования сложной функции)** позволяет вычислить производную композиции двух и более функций на основе индивидуальных производных. Если функция $f$ имеет производную в точке $x_{0}$, а функция $g$ имеет производную в точке $y_{0}=f(x_{0})$, то сложная функция $h(x)=g(f(x))$ также имеет производную в точке $x_{0}$.

В обозначениях Лейбница цепное правило для вычисления производной функции $y=y(x)$, где $x=x(t)$, принимает следующий вид:

$$\frac{dy}{dt} = \frac{dy}{dx} \cdot \frac{dt}{dt}$$


### Backpropagation
$Backpropagation$ -- метод обновления весов сеток.

[Источник того, что ниже -- википедия.](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BE%D0%B1%D1%80%D0%B0%D1%82%D0%BD%D0%BE%D0%B3%D0%BE_%D1%80%D0%B0%D1%81%D0%BF%D1%80%D0%BE%D1%81%D1%82%D1%80%D0%B0%D0%BD%D0%B5%D0%BD%D0%B8%D1%8F_%D0%BE%D1%88%D0%B8%D0%B1%D0%BA%D0%B8)

**Метод обратного распространения ошибки** — метод вычисления градиента, который используется при обновлении весов многослойного перцептрона.

Основная идея этого метода состоит в распространении сигналов ошибки от выходов сети к её входам, в направлении, обратном прямому распространению сигналов в обычном режиме работы.

#### Описание работы

Алгоритм обратного распространения ошибки применяется для многослойного перцептрона. У сети есть множество входов $x_{1},...,x_{n}$, множество выходов $Outputs$ и множество внутренних узлов. Перенумеруем все узлы (включая входы и выходы) числами от $1$ до $N$ (сквозная нумерация, вне зависимости от топологии слоёв). Обозначим через  $w_{i,j}$ вес, стоящий на ребре, соединяющем $i$-й и $j$-й узлы, а через $o_i$ — выход $i$-го узла. Если нам известен обучающий пример (правильные ответы сети $t_k$, $k \in \mathrm{Outputs}$), то функция ошибки, полученная по методу наименьших квадратов, выглядит так:

$$E(\{w_{i,j}\}) = \tfrac{1}{2} \!\! \sum_{k \in \mathrm{Outputs}} \!\!\! (t_k - o_k)^2 $$

ак модифицировать веса? Мы будем реализовывать стохастический градиентный спуск, то есть будем подправлять веса после каждого обучающего примера и, таким образом, «двигаться» в многомерном пространстве весов. Чтобы «добраться» до минимума ошибки, нам нужно «двигаться» в сторону, противоположную градиенту, то есть, на основании каждой группы правильных ответов, добавлять к каждому весу $w_{i,j}$

$$\Delta w_{i,j} = -\eta \frac {\partial E}{\partial w_{i,j}},$$

где $0 < \eta < 1$ — множитель, задающий скорость «движения»


Производная считается следующим образом. Пусть сначала $j \in \mathrm{Outputs}$, то есть интересующий нас вес входит в нейрон последнего уровня. Сначала отметим, что $w_{i,j}$ влияет на выход сети только как часть суммы  $S_j = \sum_{i} w_{i,j}x_{i}$, где сумма берётся по входам $j$-го узла. Поэтому

$$\cfrac{\partial E}{\partial w_{i,j}} = \cfrac{\partial E}{\partial S_j}\, \cfrac{\partial S_j}{\partial w_{i,j}} = x_{i} \cfrac{\partial E}{\partial S_j} $$

Аналогично, $S_j$ влияет на общую ошибку только в рамках выхода $j$-го узла $o_j$ (напоминаем, что это выход всей сети). Поэтому

$$\cfrac{\partial E}{\partial S_j} = \cfrac{\partial E}{\partial o_j}\,\cfrac{\partial o_j}{\partial S_j} = \\ =\left (\cfrac{\partial}{\partial o_j}\,\cfrac{1}{2}\sum_{k \in \mathrm{Outputs}}(t_k - o_k)^2 \right ) \!\! \left (\cfrac{\partial \operatorname{f}(S)}{\partial S}\mid_{S=S_j} \right) = \\
\\ = \left ( \cfrac{1}{2}\, \cfrac{\partial}{\partial o_j}(t_j - o_j)^2 \right) (o_j(1 - o_j)) 2\alpha = - 2 \alpha o_j(1 - o_j)(t_j - o_j),$$

где $f(S)$ — соответствующая сигмоида, в данном случае — экспоненциальная


Если же $j$-й узел — не на последнем уровне, то у него есть выходы; обозначим их через $Children(j)$. В этом случае

$$\cfrac{\partial E}{\partial S_j} = \sum_{k \in \mathrm{Children}(j)} \cfrac{\partial E}{\partial S_k}\, \cfrac{\partial S_k}{\partial S_j},$$

и

$$\cfrac{\partial S_k}{\partial S_j} = \cfrac{\partial S_k}{\partial o_j}\, \cfrac{\partial o_j}{\partial S_j} = w_{j,k} \cfrac{\partial o_j}{\partial S_j} = 2\alpha w_{j,k}o_j(1 - o_j).$$

Но $\cfrac{\partial E}{\partial S_k}$ — это в точности аналогичная поправка, но вычисленная для узла следующего уровня. Будем обозначать её через $\delta _k$ — от $\Delta _k$ она отличается отсутствием множителя $(-\eta x_{i,j})$. Поскольку мы научились вычислять поправку для узлов последнего уровня и выражать поправку для узла более низкого уровня через поправки более высокого, можно уже писать алгоритм. Именно из-за этой особенности вычисления поправок алгоритм называется алгоритмом обратного распространения ошибки ($backpropagation$).

Краткое резюме проделанной работы:
* для узла последнего уровня:
$$\delta _j = -2\alpha o_j(1 - o_j)(t_j - o_j)$$
* для внутреннего узла сети:
$$\delta _j = 2\alpha o_j(1 - o_j) \!\! \sum_{k \in \mathrm{Children}(j)} \!\! \delta _k w_{j,k}$$
* для всех узлов:
$$\Delta w_{i,j} = -\eta \delta _j o_{i},$$
где $o_{i}$ это тот же $x_{i}$ в формуле для $\cfrac{\partial E}{\partial w_{i,j}}$.

[Источник того, что ниже - лекции.](https://docs.google.com/document/d/1HXz9Q7sp50WDAd3Fuk0eqzikXN90KmClK8fkG-2y0Iw/edit#)

Итак, мы посчитали градиенты по каждому весу, но это градиенты функции ошибки, ее мы хотим минимизировать, так что веса будем двигать в сторону, противоположную градиенту. Замечание: могут встречаться оба варианта: подвигать по градиенту (если мы максимизируем какой-нибудь “gain”) или против (если минимизируем loss).
Сам алгоритм:
* инициализировать веса $w_{i, j}$ случайными величинами
* $forward\ step$ - получаем $o_j$
* $backward\ step$ - получаем $\delta_j$
* обновляем веса

Этот алгоритм лучше запускать несколько раз с разными начальными весами, потому что начиная с разных точек в пространстве весов и значения функции потерь мы можем скатываться в разные минимумы.

### Оптимизаторы

#### Adagrad

[Источник того, что ниже - хабр.](https://habr.com/post/318970/)
$Adagrad = adaptive\ gradient$


Некоторые признаки могут быть крайне информативными, но встречаться редко. Экзотическая высокооплачиваемая профессия, причудливое слово в спам-базе — они запросто потонут в шуме всех остальных обновлений. Речь идёт не только о редко встречающихся входных параметрах. Скажем, вам вполне могут встретиться редкие графические узоры, которые и в признак-то превращаются только после прохождения через несколько слоёв свёрточной сети. Хорошо бы уметь обновлять параметры с оглядкой на то, насколько типичный признак они фиксируют. Достичь этого несложно: давайте будем хранить для каждого параметра сети сумму квадратов его обновлений. Она будет выступать в качестве прокси для типичности: если параметр принадлежит цепочке часто активирующихся нейронов, его постоянно дёргают туда-сюда, а значит сумма быстро накапливается. Перепишем формулу обновления вот так:

$$G_{t} = G_{t} + g_{t}^2 \\
w_{t+1} = w_{t} - \frac{\eta}{\sqrt{G_{t} + \varepsilon}} g_{t},$$
где
$g_t = \nabla_w J( w_t ),\ \ \ J( w )$ - целевая функция,
$G_{t}$ — сумма квадратов обновлений, по-другому, сумма всех градиентов,
а $\varepsilon$ — сглаживающий параметр, необходимый, чтобы избежать деления на $0$. 

У часто обновлявшегося в прошлом параметра большая $G_t$, значит большой знаменатель во второй формуле. Параметр изменившийся всего раз или два обновится в полную силу. $\varepsilon$ берут порядка $10^{-6}$ или $10^{-8}$ для совсем агрессивного обновления, но, как видно из графиков, это играет роль только в начале, ближе к середине обучение начинает перевешивать $G_t$.

Итак, идея $Adagrad$ в том, чтобы использовать что-нибудь, что бы уменьшало обновления для элементов, которые мы и так часто обновляем. Никто нас не заставляет использовать конкретно эту формулу, поэтому Adagrad иногда называют семейством алгоритмов. Скажем, мы можем убрать корень или накапливать не квадраты обновлений, а их модули, или вовсе заменить множитель на что-нибудь вроде $e^{-G_{t}}$.

(Другое дело, что это требует экспериментов. Если убрать корень, обновления начнут уменьшаться слишком быстро, и алгоритм ухудшится)


Ещё одно _достоинство_ $Adagrad$ — отсутствие необходимости точно подбирать скорость обучения. Достаточно выставить её в меру большой, чтобы обеспечить хороший запас, но не такой громадной, чтобы алгроритм расходился. По сути мы автоматически получаем затухание скорости обучения ($learning rate decay$).

#### RMSProp

$RMSProp = root\ mean\ square\ propagation$

Недостаток $Adagrad$ в том, что $G_{t}$ в формуле обновления весов может увеличиваться сколько угодно, что через некоторое время приводит к слишком маленьким обновлениям и параличу алгоритма. $RMSProp$ и $Adadelta$ призваны исправить этот недостаток.


:point_right: Модифицируем идею $Adagrad$: мы всё так же собираемся обновлять меньше веса, которые слишком часто обновляются, но вместо полной суммы обновлений, _будем использовать усреднённый по истории квадрат градиента_. Снова используем экспоненциально затухающее бегущее среднее. Пусть  $E[g^2]_t$ — бегущее среднее в момент $t$:
$$E[g^2]_t = \gamma E[g^2]_{t-1} + (1 - \gamma) g^2_t$$

тогда формула обновления весов получится:
$$w_{t+1} = w_{t} - \frac{\eta}{\sqrt{E[g^2]_t + \varepsilon}} g_{t}$$

Знаменатель есть корень из среднего квадратов градиентов, отсюда и название метода $RMSProp — root\ mean\ square\ propagation$
$$RMS[g]_t = \sqrt{E[g^2]_t + \varepsilon}$$


#### Adadelta
$Adadelta$ отличается от $RMSProp'a$ тем, что мы добавляем в числитель формулы обновления весов стабилизирующий член пропорциональный $RMS$ от $\Delta w_t.$ 

На шаге $t$ мы ещё не знаем значение $RMS[\Delta w]_{t}$, поэтому обновление параметров происходит в три этапа, а не в два:
1. сначала накапливаем квадрат градиента;
2. затем обновляем $w$;
3. после чего обновляем $RMS[\Delta w]$.

$$\Delta w = -\frac{RMS[\Delta w]_{t-1}}{RMS[g]_{t}}g_{t}
\\
w_{t+1} = w_{t} - \frac{RMS[\Delta w]_{t-1}}{RMS[g]_{t}}g_{t}$$

Для подсчета $RMS[\Delta w]_{t}$, как и раньше, используем экспоненциально затухающее бегущее среднее:

$$E[\Delta w^2]_t = \gamma E[\Delta w^2]_{t-1} + (1 - \gamma) \Delta w^2_t
\\
RMS[\Delta w]_{t} = \sqrt{E[\Delta w^2]_t + \varepsilon}$$

Такое изменение сделано из соображений, что размерности $w$ и $\Delta w$ должны совпадать. Заметьте, что $learning rate$ не имеет размерности, а значит во всех алгоритмах до этого мы складывали размерную величину с безразмерной. Физики в этом месте ужаснутся, а мы пожмём плечами: работает же.


Заметим, что нам нужен ненулевой $RMS[\Delta w]_{-1}$ для первого шага, иначе все последующие $\Delta w$, а значит и $RMS[\Delta w]_{t}$ будут равны нулю.
Но эту проблему мы решили ещё раньше, добавив в $RMS\ \varepsilon$. Другое дело, что без явного большого $RMS[\Delta w]_{-1}$ мы получим поведение, противоположное $Adagrad$ и $RMSProp$: мы будем сильнее (до некоторого предела) обновлять веса, которые используются чаще. Ведь теперь чтобы $\Delta w$ стал значимым, параметр должен накопить большую сумму в числителе дроби.

> Впрочем, похоже, авторы алгоритма и добивались такого эффекта. Для $RMSProp$ и $Adadelta$, как и для $Adagrad$ не нужно очень точно подбирать скорость обучения — достаточно прикидочного значения. Обычно советуют начать подгон $\eta$ c $0.1 - 1$, a $\gamma$ так и оставить $0.9$. 
> 
> Чем ближе $\gamma$ к $1$, тем дольше $RMSProp$ и $Adadelta$ с большим $RMS[\Delta w]_{-1}$ будут сильно обновлять мало используемые веса. Если же $\gamma \approx 1$ и $RMS[\Delta w]_{-1} = 0$, то $Adadelta$ будет долго «с недоверием» относиться к редко используемым весам.
> 
> Последнее может привести к параличу алгоритма, а может вызвать намеренно «жадное» поведение, когда алгоритм сначала обновляет нейроны, кодирующие самые лучшие признаки.


## Вопрос 2: Speech recognition. Connectionist Temporal Classification. Deep Speech. CNN Speech Recognition.